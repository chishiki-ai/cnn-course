{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e805597",
   "metadata": {},
   "source": [
    "# Single Node MultiGPU Training with Torchrun \n",
    "\n",
    "In the previous notebook we introduced parallel computing at HPC centers and a specific algorithm for distributed training with Pytorch, DDP.  However, there are can be many challenges that arise when utilizing multiple computational resources.  One of the biggest challenges is what happens when one of the computational resources fails during training?  In this notebook we will discuss these issues and how we set up our parallel implementation to be able to continue to run despite intermittent computational resources.  We will also combine the information in this tutorial and the previous tutorial and apply it to the DesignSafe Classifier we used previously.   \n",
    "\n",
    "<center>\n",
    "<img src=\"https://impanix.com/wp-content/uploads/2023/05/What-is-Fault-Tolerance-Types-and-How-To-Implement-768x461.png\" width=400 /><br>\n",
    "<b>Figure 1.</b> Fault Tolerance \n",
    "</center>\n",
    "\n",
    "\n",
    "Specifically, in this tutorial, we will cover the following material:\n",
    "- Introduce Fault Tolerance\n",
    "- Introduce Pytorch's Torchrun\n",
    "- Go over code modifications need to use torchrun to launch distributed code that is fault tolerant\n",
    "- Implement a script for training the design safe classifier using torchrun "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80451590",
   "metadata": {},
   "source": [
    "## Fault Tolerance\n",
    "\n",
    "Leveraging multiple GPUs to train neural networks comes with many challenges.  One of the biggest is what happens when GPUs fails due to many potential issues (overheating, old system wears out, virus, etc.).  **Fault tolerance** is the ability of a system to maintain operation despite one of its components failing. One way to combat component failure is via checkpointing.  In checkpointing we periodically save the state of our application (in the case of deep learning the current weights of our model), so that if a process failure occurs we can resume our application from the previous checkpoint (See figure 2). \n",
    "\n",
    "\n",
    "<img src=\"./img/checkpointing.png\" />\n",
    "\n",
    "<b>Figure 2.</b> Visual of checkpointing.  CP refers to a point in time when a checkpoint is saved. \n",
    "\n",
    "Next, we will talk about `torchrun`, pytorch's tools for launching distributed training that will handle fault tolerance via check pointing for you. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470907f",
   "metadata": {},
   "source": [
    "## Using Torchrun \n",
    "\n",
    "Pytorch has a tool which automatically handles fault tolerance with checkpointing called `torchrun`.  Specifically, `torhcrun` has he following functionalities:\n",
    "\n",
    "-  worker failures are handled gracefully by restarting your workers at the previously saved checkpoint\n",
    "-  environment variables, like RANK and WORLD_SIZE, are automatically set for you.  All environment variables set by pytorch can be found [here](https://pytorch.org/docs/stable/elastic/run.html#environment-variables)\n",
    "-  number of nodes being leveraged can vary during training (elasticity)\n",
    "\n",
    "In this notebook we will introduce how to utilize environment variable automatically set in torchrun as well as how to use checkpointing.  We will not cover elasticity as it is outside the scope of this course.  To explain the functionality of the torchrun we will:\n",
    "\n",
    "1. Cover the code modifications needed using the MNIST example from the previous notebook\n",
    "2. Explain how to launch your script.\n",
    "\n",
    "Let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6b8f4b",
   "metadata": {},
   "source": [
    "### Code Modifications with MNIST Example\n",
    "\n",
    "To utilize torchrun's functionality we will need to make some changes to the distributed scaling script we created in the previous notebook. These code changes include:\n",
    "\n",
    "1. Modify code for environment variables set by torchrun:\n",
    "    1. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
    "    2. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
    "2. Add code for writing checkpoints and resuming training after failure\n",
    "    1. Create location to store checkpoints\n",
    "    2. Read checkpoints if they exist and resume training at epoch checkpoint was written\n",
    "    3. Write checkpoints periodically during training\n",
    "3. Remove using the mp.spawn to parallelize code and replace this with a function call, as this is done automatically by torchrun\n",
    "\n",
    "Let's highlight the listed code changes above by revisiting the MNIST example we used in previous notebook.  In order to implement these changes only two functions need to me modified, `init_distributed` and `main` functions.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52af9480",
   "metadata": {},
   "source": [
    "#### 1. Modify code for environment variables set by torchrun\n",
    "\n",
    "In order to use the environment variables set by torchrun we will need to make modifications to both the `init_distributed` and `main` functions as highlighted in code example below.  In summary, we removed the local_rank and world_size arguments from the `init_distributed` function and instead set these variables within the function from the environment variables set by torchrun. Additionally, we modify our main function to utilize the `local_rank` environment variable to set the device where our model should be stored as well as call the modified `init_distributed` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "656a60d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##################################################################################\n",
    "# A. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
    "def init_distributed():    # (local_rank, world_size):\n",
    "\n",
    "    # B. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    torch.cuda.set_device(local_rank)                       \n",
    "    dist.init_process_group(\"nccl\",                   \n",
    "                            rank=local_rank,          \n",
    "                            world_size=world_size)    \n",
    "\n",
    "def main():\n",
    "    #####################################################################\n",
    "    # 1.B We also create the variable local_rank in our main function as well as call the new init_distributed()\n",
    "    # this will be used to assign the gpu where our model should reside as highlighted below \n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    init_distributed()\n",
    "    ################################################\n",
    "    # .....\n",
    "    # instantiate network and set to local_rank device\n",
    "    net = Net().to(local_rank)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1334192",
   "metadata": {},
   "source": [
    "#### 2. Add code for writing checkpoints and resuming training after failure\n",
    "\n",
    "We need to make several modifications to the main function to incorporate writing checkpoints and resuming at a checkpoint after process failure.  These modifications are highlighted below with rows of `#` and includes line by line comments to explain why each modification was written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee27c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    init_distributed()\n",
    "\n",
    "    train_dataloader = prepare_data()\n",
    "\n",
    "    ################################################                                                 \n",
    "    # 2.A. Create location to store checkpoints\n",
    "\n",
    "    # Create directory for storing checkpointed model\n",
    "    model_folder_path = os.getcwd()+\"/output_model/\"          # create variable for path to folder for checkpoints\n",
    "    os.makedirs(model_folder_path,exist_ok=True)              # create directory for models if they do not exist\n",
    "    # create file name for checkpoint \n",
    "    checkpoint_file = model_folder_path+\"best_model.pt\"       # create filename for model checkpoint\n",
    "    ################################################\n",
    "\n",
    "    net = Net().to(local_rank)\n",
    "\n",
    "    #################################################\n",
    "    # 2B. Read checkpoints if they exist \n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)  # load previous checkpoint\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])  # set model weights to be that of the last checkpoin\n",
    "        epoch_start = checkpoint['epoch']                      # set epoch where training should resume\n",
    "   \n",
    "    # otherwise we are starting training from the beginning at epoch 0\n",
    "    else:\n",
    "        epoch_start = 0\n",
    "    ################################################\n",
    "\n",
    "    model = DDP(net,\n",
    "            device_ids=[local_rank],                  # list of gpu that model lives on \n",
    "            output_device=local_rank,                 # where to output model\n",
    "        )\n",
    "\n",
    "\n",
    "    loss_fn = torch.nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    save_every = 1\n",
    "    epochs = 10\n",
    "    ###########################################################\n",
    "    # 2C. Resume training at epoch last checkpoint was written\n",
    "    for epoch in range(epoch_start, epochs):                  # note we start loop at epoch_start defined in code above\n",
    "    ###########################################################\n",
    "        train_loop(rank, train_dataloader, model, loss_fn, optimizer)\n",
    "        ###########################################################\n",
    "        # 2D. Write checkpoints periodically during training\n",
    "        if rank == 0 and epoch%save_every==0:\n",
    "            print(f\"Epoch {epoch+1}\\n-------------------------------\")\n",
    "            torch.save({                                     # save model's state_dict and current epoch periodically\n",
    "                'epoch':epoch,\n",
    "                'model_state_dict':model.module.state_dict(),\n",
    "            }, checkpoint_file)\n",
    "            print(\"Finished saving model\\n\")\n",
    "        ###########################################################\n",
    "\n",
    "    dist.destroy_process_group()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f4789",
   "metadata": {},
   "source": [
    "You can find the entire modified script with the changes highlighted above in the file `mnist_torchrun.py`.  Next, we will learn how to run this script with `torchrun`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6887b5",
   "metadata": {},
   "source": [
    "### Launching jobs with Torchrun\n",
    "\n",
    "In order to launch our new `mnist_torchrun.py` script you can use the `torchrun` command.  There are several arguments that could pass with torchrun.  These arguments vary based on the type of job you are launching.  For example, the arguments needed for a single node job versus a multinode.  For now, we will cover the arguments needs for a single node job.  \n",
    "\n",
    "Let's start by introducing three arguments that can be helpful when launching a single node job:\n",
    "- **--standalone** : This indicates to pytorch that you are running a single machine multiworker job.  It automatically sets up a rendezvous backend that is represented by a C10d TCP store on port 29400\n",
    "- **--nnodes** : Total number of nodes being used\n",
    "- **--nproc-per-node** : number of processes per node; this is typically set to the number of GPUs on your machine(s)\n",
    "\n",
    "To launch a generic training script (YOUR_TRAINING_SCRIPT.py) on a single node with 4 GPUs you can do the following:\n",
    "\n",
    "```\n",
    "torchrun\n",
    "    --standalone\n",
    "    --nnodes=1\n",
    "    --nproc-per-node=4\n",
    "    YOUR_TRAINING_SCRIPT.py (--arg1 ... train script args...)\n",
    "```\n",
    "\n",
    "Next, let's run our MNIST training script with torchrun:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be49f48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!torchrun --nproc-per-node=4 mnist_torchrun.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e315261",
   "metadata": {},
   "source": [
    "## Exercise (optional)\n",
    "\n",
    "Modify simple linear regression script you created in previous tutorial to be able to use torchrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f086af74",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "6ea86fe9",
   "metadata": {},
   "source": [
    "## DesignSafe Classifier \n",
    "<center>\n",
    "<img src=\"https://converge.colorado.edu/wp-content/uploads/2021/06/designsafe_logo_tagline.png\" />\n",
    "</center>\n",
    "In this tutorial and the previous, we have introduced a lot of content on how to write training scripts that run on multiple GPUs with the MNIST dataset.  For the rest of this notebook we will return to an more interesting application, the DesignSafe Classifier we created in previous tutorials.  As a reminder, this is a dataset from Hurricane Harvey, a category 4 hurricane that hit Texas in August of 2017 and resulted in catastrophic flooding to the Houston metropolitan area. The data set is specifically focused on image classification of homes according to the amount of damage the home received. All images of homes are labeled as C0, C2, or C4 respectively for low, medium or high damage.\n",
    "\n",
    "\n",
    "For the remainder of this notebook we will do a code walk through for the Designsafe Classifier that is setup to run on multiple GPUs with torchrun.  Additionally this example will incorporate code for model evaluation.  The script we create in this notebook will be used in the next tutorial where we cover multinode training. Let's dive in!  \n",
    "\n",
    "### Reused code from Part 1 and 2 \n",
    "\n",
    "Pytorch aims to make code non-intrusive in that you can take existing code which trains a neural net on a single processer an easily scale out when hitting resource limitations with minimal code changes.  With this spirit in mind, let's first point out all the code that can be reused.  Below are a set of functions and import statements that we will not need to modify that were created previously. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145a9603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "import gc\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import torch.nn as nn\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f28b181a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply transformations to our data.\n",
    "# The datasets transformations are the same as the ones from part 2 of this tutorial.\n",
    "def load_datasets(train_path, val_path, test_path):\n",
    "    val_img_transform = transforms.Compose([transforms.Resize((244,244)),\n",
    "                                             transforms.ToTensor()])\n",
    "    train_img_transform = transforms.Compose([transforms.AutoAugment(),\n",
    "                                               transforms.Resize((244,244)),\n",
    "                                               transforms.ToTensor()])\n",
    "    train_dataset = datasets.ImageFolder(train_path, transform=train_img_transform)\n",
    "    val_dataset = datasets.ImageFolder(val_path, transform=val_img_transform)\n",
    "    test_dataset = datasets.ImageFolder(test_path, transform=val_img_transform) if test_path is not None else None\n",
    "\n",
    "    return train_dataset, val_dataset, test_dataset\n",
    "\n",
    "# Building the Neural Network\n",
    "def getResNet():\n",
    "    resnet = models.resnet34(weights='IMAGENET1K_V1')\n",
    "\n",
    "    # Fix the conv layers parameters\n",
    "    for conv_param in resnet.parameters():\n",
    "        conv_param.require_grad = False\n",
    "\n",
    "    # get the input dimension for this layer\n",
    "    num_ftrs = resnet.fc.in_features\n",
    "\n",
    "    # build the new final mlp layers of network\n",
    "    fc = nn.Sequential(\n",
    "          nn.Linear(num_ftrs, num_ftrs),\n",
    "          nn.ReLU(),\n",
    "          nn.Linear(num_ftrs, 3)\n",
    "        )\n",
    "   \n",
    "    # replace final fully connected layer\n",
    "    resnet.fc = fc\n",
    "    return resnet\n",
    "\n",
    "# Model evaluation.\n",
    "@torch.no_grad()\n",
    "def eval_model(data_loader, model, loss_fn, DEVICE):\n",
    "    model.train(False)\n",
    "    model.eval()\n",
    "    loss, accuracy = 0.0, 0.0\n",
    "    n = len(data_loader)\n",
    "\n",
    "    #  local_rank = int(os.environ['LOCAL_RANK'])\n",
    "\n",
    "    for i, data in enumerate(data_loader):\n",
    "        x,y = data\n",
    "        x,y = x.to(DEVICE), y.to(DEVICE)\n",
    "        pred = model(x)\n",
    "        loss += loss_fn(pred, y)/len(x)\n",
    "        pred_label = torch.argmax(pred, axis = 1)\n",
    "        accuracy += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "    return loss/n, accuracy/n\n",
    "\n",
    "# loading checkpoint\n",
    "def load_checkpoint(checkpoint_path, DEVICE):\n",
    "    checkpoint = torch.load(checkpoint_path, map_location=DEVICE)\n",
    "    return checkpoint\n",
    "\n",
    "def load_model_fm_checkpoint(checkpoint, primitive_model):\n",
    "    primitive_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    return primitive_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c570bd3",
   "metadata": {},
   "source": [
    "Next, let's highlight the modifications that need to be made to scale our DesignSafe Classifier code in the following order.  As a reminder below are the 8 modifications we highlighted in the this and the previous tutorial.\n",
    "\n",
    "**Serial to Parallel Modifications**\n",
    "\n",
    "1. Create a process group\n",
    "2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "3. Wrap Model with Pytorch's DistributedDataParallel\n",
    "4. Modify Training Loop to write model from one GPU\n",
    "5. Close process group\n",
    "\n",
    "**Torchrun Modifications**\n",
    "\n",
    "6. Modify code for environment variables set by torchrun:\n",
    "    1. Remove code that sets environment variables as this done for you automatically with torchrun.\n",
    "    2. Instead, use these environment variables set by pytorch and instead of explicitly defining them.\n",
    "7. Add code for writing checkpoints and resuming training after failure\n",
    "    1. Create location to store checkpoints\n",
    "    2. Read checkpoints if they exist and resume training at epoch checkpoint was written\n",
    "    3. Write checkpoints periodically during training\n",
    "8. Remove using the mp.spawn to parallelize code and replace this with a function call, as this is done automatically by torchrun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d665d11",
   "metadata": {},
   "source": [
    "### Setup Process Group (1 and 6)\n",
    "\n",
    "Similiar to what, we say with the MNIST example, we create a function for creating the process group.  This function includes modifications we made needed to use environment variables set by torchrun as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687ee14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06f64ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code from script\n",
    "def init_distributed():\n",
    "    '''\n",
    "    set up process group with torchrun's environment variables\n",
    "    '''\n",
    "    dist_url = \"env://\"\n",
    "    world_size = int(os.environ['WORLD_SIZE'])\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    dist.init_process_group(backend=\"nccl\", #\"nccl\" for using GPUs, \"gloo\" for using CPUs\n",
    "                          init_method=dist_url,\n",
    "                          world_size=world_size,\n",
    "                          rank=local_rank)\n",
    "    torch.cuda.set_device(local_rank)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc61024",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler (2)\n",
    "\n",
    "Next we modify the dataloader such that we are using the DistributedSampler \n",
    "- load data across gpus\n",
    "- The sampler returns a iterator over indices, which are fed into dataloader\n",
    "\n",
    "Note that we have set up a the DistributedSampler and Dataloader for our training and validation data as well. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ca840b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12044b21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_dataloaders(train_set, val_set, test_set, batch_size, shuffle=True):\n",
    "\n",
    "    ##########################################################################################\n",
    "    # 2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "\n",
    "    # create distributedsampler for train, validation and test sets\n",
    "    train_sampler = DistributedSampler(dataset=train_set,shuffle=shuffle)\n",
    "    val_sampler = DistributedSampler(dataset=val_set, shuffle=False)\n",
    "    test_sampler = DistributedSampler(dataset=test_set, shuffle=False) if test_set is not None else None\n",
    "\n",
    "    # pass distributedsampler for train, validation and test sets into DataLoader\n",
    "    train_dataloader = torch.utils.data.DataLoader(train_set,batch_size=batch_size,sampler=train_sampler,num_workers=4,pin_memory=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val_set,batch_size=batch_size,sampler=val_sampler,num_workers=4)\n",
    "    test_dataloader = torch.utils.data.DataLoader(test_aset, batch_size, sampler=test_sampler,num_workers=4) if test_set is not None else None\n",
    "\n",
    "    return train_dataloader, val_dataloader, test_dataloader\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2518ab26",
   "metadata": {},
   "source": [
    "### Write Checkpoints periodically during training and only from one device (4, 7C)\n",
    "\n",
    "Major code modifications are highlighted with comments at the end of the function below.  Note, we implement checkpointing a little differently in this script. Below we save the most recent checkpoint only if it reachs a minimum validation accuracy.  That way we will always have the most accurate model at the end of training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b78aa87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(train_loader, val_loader, model, opt, scheduler, loss_fn, epochs, DEVICE, checkpoint_file, prev_best_val_acc):\n",
    "    n = len(train_loader)\n",
    "\n",
    "    best_val_acc = torch.tensor(0.0).cuda() if prev_best_val_acc is None else prev_best_val_acc\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train(True)\n",
    "\n",
    "        train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        avg_loss, val_loss, val_acc, avg_acc  = 0.0, 0.0, 0.0, 0.0\n",
    "\n",
    "        start_time = datetime.now()\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x, y = x.to(DEVICE), y.to(DEVICE)\n",
    "            pred = model(x)\n",
    "            loss = loss_fn(pred,y)\n",
    "\n",
    "            opt.zero_grad()\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "\n",
    "            avg_loss += loss.item()/len(x)\n",
    "            pred_label = torch.argmax(pred, axis=1)\n",
    "            avg_acc += torch.sum(pred_label == y)/len(x)\n",
    "\n",
    "        val_loss, val_acc = eval_model(val_loader, model, loss_fn, DEVICE)\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        total_time = torch.tensor((end_time-start_time).seconds).cuda()\n",
    "\n",
    "        # Learning rate reducer takes action\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        avg_loss, avg_acc = avg_loss/n, avg_acc/n\n",
    "\n",
    "        ###############################################################################\n",
    "        # 4. Modify Training Loop to write model from one GPU     #####################\n",
    "        # 7C. Write checkpoints periodically throughout training. #####################\n",
    "        local_rank = int(os.environ['LOCAL_RANK'])\n",
    "        # Only machine rank==0 (master machine) saves the model and prints the metrics    \n",
    "        if local_rank == 0:\n",
    "\n",
    "          # Save the best model that has the highest val accuracy\n",
    "          if val_acc.item() > best_val_acc.item():\n",
    "            print(f\"\\nPrev Best Val Acc: {best_val_acc} < Cur Val Acc: {val_acc}\")\n",
    "            print(\"Saving the new best model...\")\n",
    "            torch.save({\n",
    "                    'epoch':epoch,\n",
    "                    'machine':local_rank,\n",
    "                    'model_state_dict':model.module.state_dict(),\n",
    "                    'accuracy':val_acc,\n",
    "                    'loss':val_loss\n",
    "            }, checkpoint_file)\n",
    "            best_val_acc = val_acc\n",
    "            print(\"Finished saving model\\n\")\n",
    "\n",
    "          # Print the metrics (should be same on all machines)\n",
    "          print(f\"\\n(Epoch {epoch+1}/{epochs}) Time: {total_time}s\")\n",
    "          print(f\"(Epoch {epoch+1}/{epochs}) Average train loss: {avg_loss}, Average train accuracy: {avg_acc}\")\n",
    "          print(f\"(Epoch {epoch+1}/{epochs}) Val loss: {val_loss}, Val accuracy: {val_acc}\")\n",
    "          print(f\"(Epoch {epoch+1}/{epochs}) Current best val acc: {best_val_acc}\\n\")\n",
    "        ###############################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e2f95fb",
   "metadata": {},
   "source": [
    "### Create Clean Up Function (5)\n",
    "\n",
    "This function simply closes the process group at the end of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c337fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cleanup():\n",
    "    print(\"Cleaning up the distributed environment...\")\n",
    "    dist.destroy_process_group()\n",
    "    print(\"Distributed environment has been properly closed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa4a96c",
   "metadata": {},
   "source": [
    "### Wrap Model with DDP and put everything together in main function (3, 6B, 7A, 7B)\n",
    "\n",
    "In the main function we wrap our model with ddp, use pytorch's environment varaibles to specify our device, and create what's needed to store and resume training at checkpoints.  \n",
    "\n",
    "Additionally, we report on the best model we find throughout training at the end of the script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e30e5671",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    hp = {\"lr\":1e-4, \"batch_size\":16, \"epochs\":5}\n",
    "    train_path, val_path, test_path = \"/tmp/Dataset_2/Train/\", \"/tmp/Dataset_2/Validation/\", None\n",
    "\n",
    "    #################################################\n",
    "    # 6B. Use pytorch's enviornment variables.  #####\n",
    "    local_rank = int(os.environ['LOCAL_RANK'])\n",
    "    #################################################\n",
    "    DEVICE = torch.device(\"cuda\", local_rank)\n",
    "\n",
    "    ###########################################################\n",
    "    # 7A. create location to store checkpoints if they exist. ##\n",
    "    model_folder_path = os.getcwd()+\"/output_model/\"\n",
    "    os.makedirs(model_folder_path,exist_ok=True)\n",
    "    ###########################################################\n",
    "   \n",
    "    loss_fn = nn.CrossEntropyLoss(label_smoothing=0.1).cuda()\n",
    "    train_set, val_set, test_set = load_datasets(train_path, val_path, test_path)\n",
    "    train_dataloader, val_dataloader, test_dataloader = construct_dataloaders(train_set, val_set, test_set, hp[\"batch_size\"], True)\n",
    "\n",
    "    model = getResNet().to(DEVICE)\n",
    "    ##########################################################################\n",
    "    # 3. Wrap model with DDP #################################################\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[local_rank])\n",
    "    ##########################################################################\n",
    "    opt = torch.optim.Adam(model.parameters(),lr=hp[\"lr\"])\n",
    "\n",
    "    ######################################################################################\n",
    "    # 7B, Read check point if it exists and pass to the train function to resume training##\n",
    "    prev_best_val_acc = None\n",
    "    checkpoint_file = model_folder_path+\"best_model.pt\"\n",
    "    if os.path.exists(checkpoint_file):\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        prev_best_val_acc = checkpoint['accuracy']\n",
    "        model = load_model_fm_checkpoint(checkpoint,model)\n",
    "        epoch_start = checkpoint['epoch']\n",
    "        if rank == 0:\n",
    "            print(f\"resuming training from epoch {epoch_start}\")\n",
    "        else:\n",
    "            epoch_start = 0\n",
    "  ######################################################################################\n",
    "\n",
    "    # same learning rate scheduler as part 2\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(opt, mode='min',factor=0.1, patience=5, min_lr=1e-8, verbose=True)\n",
    "\n",
    "    train(train_dataloader, val_dataloader, model, opt, scheduler, loss_fn, hp[\"epochs\"], DEVICE, checkpoint_file, prev_best_val_acc)\n",
    "\n",
    "    # only the node with rank 0 does the loading, evaluation and printing to avoild duplicate \n",
    "    if local_rank == 0:\n",
    "        # store and print info on the best model at the end of training\n",
    "        primitive_model = getResNet().to(DEVICE)\n",
    "        checkpoint = load_checkpoint(checkpoint_file, DEVICE)\n",
    "        best_model = load_model_fm_checkpoint(checkpoint,primitive_model)\n",
    "        loss, acc = eval_model(val_dataloader,best_model,loss_fn,DEVICE)\n",
    "        print(f\"\\nBest model (val loss: {loss}, val accuracy: {acc}) has been saved to {checkpoint_file}\\n\")\n",
    "        ###############################\n",
    "        # 5. close process group ######\n",
    "        cleanup()\n",
    "        ###############################"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5846857",
   "metadata": {},
   "source": [
    "Finally, let's run our designsafe classifier on a single node and 4 GPUs. Well start by copying the data that we need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861757c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train  Validation\r\n"
     ]
    }
   ],
   "source": [
    "! cp -r /scratch1/07980/sli4/training/cnn_course/data/data.tar.gz /tmp/\n",
    "! tar zxf /tmp/data.tar.gz -C /tmp\n",
    "! ls /tmp/Dataset_2\n",
    "! rm /tmp/data.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e66a08a",
   "metadata": {},
   "source": [
    "Then, launch the job with torchrun."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "985c35c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 1322, Validation set size: 363\n",
      "resuming training from epoch 1\n",
      "\n",
      "(Epoch 2/5) Time: 40s\n",
      "(Epoch 2/5) Average train loss: 0.03673231300950308, Average train accuracy: 0.8292748928070068\n",
      "(Epoch 2/5) Val loss: 0.060010798275470734, Val accuracy: 0.669507622718811\n",
      "(Epoch 2/5) Current best val acc: 0.761363685131073\n",
      "\n",
      "\n",
      "Prev Best Val Acc: 0.761363685131073 < Cur Val Acc: 0.7670454978942871\n",
      "Saving the new best model...\n",
      "Finished saving model\n",
      "\n",
      "\n",
      "(Epoch 3/5) Time: 42s\n",
      "(Epoch 3/5) Average train loss: 0.031689850097417316, Average train accuracy: 0.886904776096344\n",
      "(Epoch 3/5) Val loss: 0.04760196432471275, Val accuracy: 0.7670454978942871\n",
      "(Epoch 3/5) Current best val acc: 0.7670454978942871\n",
      "\n",
      "\n",
      "(Epoch 4/5) Time: 42s\n",
      "(Epoch 4/5) Average train loss: 0.02868088988740465, Average train accuracy: 0.9166666865348816\n",
      "(Epoch 4/5) Val loss: 0.04517853260040283, Val accuracy: 0.7566288709640503\n",
      "(Epoch 4/5) Current best val acc: 0.7670454978942871\n",
      "\n",
      "\n",
      "Prev Best Val Acc: 0.7670454978942871 < Cur Val Acc: 0.7982954978942871\n",
      "Saving the new best model...\n",
      "Finished saving model\n",
      "\n",
      "\n",
      "(Epoch 5/5) Time: 40s\n",
      "(Epoch 5/5) Average train loss: 0.024220123725794095, Average train accuracy: 0.9672619104385376\n",
      "(Epoch 5/5) Val loss: 0.04160759598016739, Val accuracy: 0.7982954978942871\n",
      "(Epoch 5/5) Current best val acc: 0.7982954978942871\n",
      "\n",
      "\n",
      "Best model (val loss: 0.04160759598016739, val accuracy: 0.7982954978942871) has been saved to /home1/07980/sli4/cnn-course/scaling_part2/output_model/best_model.pt\n",
      "\n",
      "Cleaning up the distributed environment...\n",
      "Distributed environment has been properly closed\n"
     ]
    }
   ],
   "source": [
    "!torchrun --nproc-per-node=4 torch_train_distributed.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d5cf33",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we covered the basics of how Distributed Data Parallel (DDP) works, highlighted major code modifications needed to convert a nondistributed script into a distributed training script, and made these modifications for the DesignSafe Image Classifier example.  You can find the entire script created in the notebook here: `torch_train_distributed.py`. In the next section, we will discuss how we can launch this script to leverage a single and multiple nodes on HPC systems. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7225b2ec",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. https://arxiv.org/abs/2006.15704\n",
    "2. https://pytorch.org/tutorials/beginner/ddp_series_theory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21457a0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnn_course_container",
   "language": "python",
   "name": "cnn_course_container"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
