{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98e219b8",
   "metadata": {},
   "source": [
    "# Introduction to DDP with Pytorch\n",
    "\n",
    "Training neural networks is an expensive computational task that is getting even more expensize.  In figure [1] (obtained from Reference [1]), we can see how the size of neural networks have grown exponentially over time.  As the number of parameters increase the training times also increase. Using multiple GPUs is critical for training deep learning models. \n",
    "\n",
    "<img src=\"./img/size_networks.png\" />\n",
    "\n",
    "The aim of this tutorial is to introduce the basics of parallel computation and implement a method called distributed data parallel with Pytorch that runs on one node with multiple GPUs. Specifically, we will cover the following material:\n",
    "\n",
    "- Introduce parallel computing at HPC centers\n",
    "- Introduce the basics of Distributed Data Parallel (DDP) \n",
    "- Highlight major code modifications needed to scale non-distributed model training scripts with Pytorch's DDP\n",
    "- Modify code from a simple MNIST Classifier example to run at scale using DDP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65de4b4",
   "metadata": {},
   "source": [
    "## Introduction to Parallel Computing on HPC\n",
    "\n",
    "<center>\n",
    "<img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Flemppics.lemp.io%2F1665137786138.png&f=1&nofb=1&ipt=6385ec8c79507458c881bb4706bf1603dec5b5ced981ca2efe6dcd4d085da2bf&ipo=images\" width=\"300\"><br>\n",
    "<b>Figure 2.</b> Serial versus Parallel Computation\n",
    "<br>\n",
    "</center>\n",
    "Traditionally, software has been written for serial computation where we solve a problem by executing a set of instructions sequentially on a single processor.  In parallel computation, multiple computational resources are leveraged simulateously to solve a problem and thus requires multiple processers to run. There are many problems in science and engineering the require parallel computation that run on computational resources beyond those available on our laptops, including training neural networks.  High Performance Computing (HPC) centers are a resource we can use to get access to potentially thousands of computer to execute code.    \n",
    "\n",
    "<center>\n",
    "<img src=\"https://docs.tacc.utexas.edu/basics/imgs/login-compute-nodes.jpg\" width=\"500\"><br>\n",
    "<b>Figure 3.</b> Login vs Compute Nodes\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "HPC centers, like the texas advanced computing center (TACC), host several supercomputers.  Supercomputers, or computer clusters, are a group of interconnected computers such that they can act like a single machine.  The various computers that make up the computer cluster are **nodes** which come in two types: login and compute.  Login nodes are those that you interact with in logging on to our machines via [SSH](https://www.geeksforgeeks.org/introduction-to-sshsecure-shell-keys/) and are used for routine task like modifying and organizing files.  Compute nodes are where the actual calculations are done and what we will utilize to parallelize the training of neural networks.  There could be different types of compute nodes, for example different types of CPU and GPUs, but we will focus on utilizing GPUs in this tutorial as Pytorch is optimized to run on GPUs.  A GPU node typically consists of multiple GPUs.  Each GPU on a node is identified with a unique integer referred to as **local rank** (See figure 4).  \n",
    "\n",
    "\n",
    "<img src=\"./img/local_rank_global_rank.png\" />\n",
    "\n",
    "<b>Figure 4.</b> Local Rank\n",
    "\n",
    "\n",
    "Pytorch has tools for checking the number and types of GPUs available.  For example, you can check if GPUs are avaiable with `is_available()`. You can determine the number of GPUs you have available via `device_count()`. You can also determine the local rank of the device you are currently using with `current_device()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6718ad7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85079b05",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('GPUs are available = {}'.format(torch.cuda.is_available()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de0e2ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of GPUs available are {}'.format(torch.cuda.device_count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c9389d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('You are currently using GPU with local rank = {}'.format(torch.cuda.current_device()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0820bb",
   "metadata": {},
   "source": [
    "In this notebook, we will introduce how to parallelize the training process of a CNN classifier across multiple GPUs on a single node.  One of the main challenges of parallelising code is that there is often a need for the various processes to communicate with one another. Let's say for example I want to solve the following set of equations: \n",
    "\n",
    "$$A = B * C$$\n",
    "$$D = E * F$$\n",
    "$$G = A * D$$\n",
    "\n",
    "One way to parallize solving this simple set of equations is to use two processors to compute $A$ and $D$.  There are a few things that need  to happen after $A$ and $D$ are computed on the 2 processors: \n",
    "\n",
    "1. **Synchronization:** Both processes wait until all members of the group have reached the synchronization point. Both $A$ and $D$ need to both have been computed to move on to computing $G$.  Synchronization can be a time bottleneck in parallel computing. \n",
    "2. **Data Movement** - Once $A$ and $D$ have been been computed, the values of A and D need to reside on the same processor.  This is where data movement comes in (see figure 5 for examples of different types of data movement). \n",
    "3. **Collective Computation** - In this example one processor collects data (values of A and D) and performs an operation (min, max, add, multiply, etc.) on that data.\n",
    "\n",
    "<center>\n",
    "<img src=\"https://hpc-tutorials.llnl.gov/mpi/images/collective_comm.gif\" width=\"400\"><br>\n",
    "<b>Figure 5.</b> Types of Data Movement\n",
    "<br>\n",
    "</center>\n",
    "\n",
    "All 3 of these steps typically need to be programmed when building parallelized code and can often be time bottlenecks. \n",
    "\n",
    "Now that we have introduced a little bit about parallel computation, HPC terminology and how communication works, let's look at one algorithm which is used to parallelize the training of neural networks, Distributed Data Parallel (DDP).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c27315",
   "metadata": {},
   "source": [
    "## Distributed Data Parallel (DDP)\n",
    "\n",
    "Before we dive into how DDP works, let's review what the typical training process entails using a single GPU.  One step in the process of training a neural network consist of the following:\n",
    "\n",
    "1. Receive an input batch and complete a forward pass to compute the loss associated with your current weights\n",
    "2. Based of of [1], complete a backward pass where gradients associated with your current weights and batch are computed\n",
    "3. Update weights based on gradients from [2] and selected optimization algorithm\n",
    "\n",
    "Steps 1-3 described above and visually represented in figure 6 are repeated iteratively until a minimal loss is achieved or computing resources run out. \n",
    "\n",
    "\n",
    "<img src=\"./img/singlegpu.png\" />\n",
    "\n",
    "<b>Figure 6.</b>  Visual representation of one step in training a neural metwork with one GPU.\n",
    "\n",
    "One popular technique to scale the training process, or modify the training process so that we are leveraging multiple GPUs, is called distirbuted data parallel (DDP).  In DDP, each GPU being used launches one process where each GPU contains a local copy of the model being optimized. Then, a different randomly generated batch of data is sent to each GPU and the forward and backward passes are computed.   \n",
    "\n",
    "One option for the next step would be to updated model weights on each GPU.  However, this would result in different model weights as the loss and gradients computed should all be different on each GPU as different data was used.  Instead, all gradients are synchronized by averaging  gradients from each GPU and sent back to the individual GPUs via an Allreduce operation. The Allreduce operation is where the synchronization, data movement and collective communications come into play in DDP:\n",
    "\n",
    "1. Synchronization: wait until all GPUs have computed their gradients\n",
    "2. Data Movement: Move data so that average gradient can be computed and then broadcast back to all GPUs.\n",
    "3. Collective Computation: averaging gradients from each gpu\n",
    "\n",
    "Performing the Allreduce operation of the gradients ensures that when the weights are updated on each GPU, that the models remain equivalent. This process is repeated throughout the training process with DDP. A visual representation of one iteration of DDP is shown in figure 7.\n",
    "\n",
    "\n",
    "<img src=\"./img/multigpu.png\" width=\"600\"><br>\n",
    "\n",
    "<b>Figure 7.</b> Visual representation of one iteration of DDP using 3 GPUs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c5f5f2",
   "metadata": {},
   "source": [
    "## Introduction to DDP with Pytorch\n",
    "\n",
    "Next, we will introduce how to convert a non-distributed training script into a distributed training script that uses Pytorch's DDP. The  major modifications needed to scale a Pytorch training script are as follows: \n",
    "\n",
    "1. Create a process group\n",
    "2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "3. Wrap Model with Pytorch's DistributedDataParallel\n",
    "4. Modify Training Loop to write model from one GPU\n",
    "5. Close process group\n",
    "\n",
    "Next, let's dive into each of the modifications above in more detail. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a788c0",
   "metadata": {},
   "source": [
    "### Create Process Group \n",
    "\n",
    "The first step to scaling a pytorch script is to set up a process group -- that is establishing the group of processors that will be used so that the processors can communicate.  This can be done via the `torch.distributed.init_process_group` method. \n",
    "\n",
    "Below is an example of how to set up the process group locally.  First, we set environment variables for the IP address for the rank 0 process and a free port.  Later on in this tutorial we will give an example of how this can be set up for an HPC cluster. \n",
    "\n",
    "Note, we set the device prior to setting up the process group. This is important to prevent hangs or excessive mememory utilization. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c230af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                       #\n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,                # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d45e96",
   "metadata": {},
   "source": [
    "### Create Data DistributedSampler\n",
    "\n",
    "The purpose of the DistributedSampler is to distribute data amongst the various processes. It ensures that the batch that each GPU receives is different. The distributed sampler passes an iterator that sends data to the various processes. \n",
    "\n",
    "In the code snippet below, we use the DataLoader as we saw in the previous notebook, but we pass the `DistributedSampler` via the sampler argument.  We also change shuffle from True to False. Note, in the example below each GPU would receive a batch size of 32 data samples.  Thus, the actual batch size would be number_gpu * 32.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f06f692d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def load_dataset(train_dataset):\n",
    "    train_data = torch.utils.data.DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=32,\n",
    "        #################################################\n",
    "        shuffle=False,                             # shuffle should be set to False when using DistributedSampler\n",
    "        sampler=DistributedSampler(train_dataset), # passing the distributed loader\n",
    "        ################################################\n",
    "    )\n",
    "    return train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd09bcf7",
   "metadata": {},
   "source": [
    "### Wrap Model with DistributedDataParallel\n",
    "\n",
    "In order to use Pytorch's Distributed Data Parallel we need to wrap our model (e.g. resnet18) with the `DDP` wraper.  In the function below we combine instantiating our process group, setting up the distributed sampler, and wrapping our model with DDP into one function. \n",
    "\n",
    "```python\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def main(local_rank, world_size):\n",
    "    # setup the process groups\n",
    "    setup(local_rank, world_size)    \n",
    "    # prepare the dataloader with the DistributedSampler\n",
    "    train_data = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                             batch_size=32,\n",
    "                                             shuffle=True,\n",
    "                                             shuffle=False,\n",
    "                                             ##########################################\n",
    "                                             sampler=DistributedSampler(train_dataset),\n",
    "                                             ###########################################\n",
    "                                            )\n",
    "    \n",
    "    # instantiate the model(in our case resnet18) and move it to the right device\n",
    "    model = models.resnet18(weights=\"IMAGENET1K_V1\") .to(local_rank)\n",
    "\n",
    "    ###############################\n",
    "    # wrap the model with DDP   \n",
    "    model = DDP(model, \n",
    "                device_ids=[local_rank],                  # list of gpu that model lives on \n",
    "                output_device=local_rank,                 # where to output model\n",
    "                )        \n",
    "    ###############################\n",
    "```\n",
    "\n",
    "Note that when we wrap our model with DDP we will need to modify other code where  access that state of our model.  Previously when we wanted to access a model's `.state_dict()` we would do this by calling the following method of our model:\n",
    "\n",
    "```python\n",
    "model.state_dict()\n",
    "```\n",
    "\n",
    "When the model has been wrapped with DDP we would need to make the following modification:\n",
    "\n",
    "```python\n",
    "model.modules.state_dict()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0dbbb51",
   "metadata": {},
   "source": [
    "### Train Model \n",
    "\n",
    "When saving our model's checkpoints throughout the training process and using DDP, by default models on each GPU are saved.  This is unnecessarily redudant.  To fix this we can modify our train function to only save our model's state_dict from one of our GPUs. The pseudocode below highlights this modification. \n",
    "\n",
    "```python\n",
    "def train(..):\n",
    "    rank = int(os.environ['RANK'])\n",
    "    for epoch in epochs:\n",
    "        for batch in batches:\n",
    "            step(...). # forward pass, backward pass, update weights\n",
    "        ###################################################\n",
    "        # only save model state from one GPU \n",
    "        if rank == 0:\n",
    "        ###################################################\n",
    "            torch.save({\n",
    "                    'epoch':epoch,\n",
    "                    'machine':local_rank,\n",
    "                    'model_state_dict':model.module.state_dict(),\n",
    "                    'accuracy':val_acc,\n",
    "                    'loss':val_loss\n",
    "            }, checkpoint_file)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69fa2e2f",
   "metadata": {},
   "source": [
    "### Clean Up Process Group\n",
    "\n",
    "Once, we have trained our model we can destroy the process group using the function below. \n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "\n",
    "def cleanup():\n",
    "    dist.destroy_process_group()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbeee31",
   "metadata": {},
   "source": [
    "### Summary \n",
    "\n",
    "In the above text we highlighted the major modifications that needed to scale a pytorch script to multiple GPUs. Next, we will code a simple neural network training script and modify it to use DDP.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c81dd6a",
   "metadata": {},
   "source": [
    "## MNIST Example\n",
    "\n",
    "Let's start by creating code that trains a classifier for the MNIST dataset.  We will then modify this code to run in parallel. For simplicity we will leave out code that evalaute our model with testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8118a4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0697029b",
   "metadata": {},
   "source": [
    "### Non-Distributed Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4f8c27",
   "metadata": {},
   "source": [
    "#### Get Data\n",
    "\n",
    "In the function below we dowload the MNIST dataset and pass it to a dataloader."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da82e4a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(batch_size=32):\n",
    "\n",
    "    # download MNIST dataset\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=\"data\",                                        # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "    # pass dataset to the dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  shuffle=False,\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "trainloader=prepare_data(batch_size=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b13c3fb",
   "metadata": {},
   "source": [
    "Next, let's visualize a few images from the MNIST dataset.  If you are unfamiliar with the MNIST data set you can learn more about it [here](https://en.wikipedia.org/wiki/MNIST_database)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b26697fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def imshow(img):\n",
    "    npimg = img.numpy()\n",
    "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# get some random training images\n",
    "dataiter = iter(trainloader)\n",
    "images, labels = next(dataiter)\n",
    "\n",
    "# show images\n",
    "imshow(torchvision.utils.make_grid(images))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80949c99",
   "metadata": {},
   "source": [
    "#### Build network\n",
    "\n",
    "Next, we build a network that will be used to train our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8e1a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.flatten = torch.nn.Flatten()\n",
    "        self.linear_relu_stack = torch.nn.Sequential(\n",
    "            torch.nn.Linear(28*28, 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Dropout(0.2),\n",
    "            torch.nn.Linear(128, 10),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        prob = self.linear_relu_stack(x)\n",
    "        return prob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0769bcdf",
   "metadata": {},
   "source": [
    "#### Train Model \n",
    "\n",
    "Below we create two functions.  The first called `train_loop` performs an epoch in the training process.  The second function called `main` does everything we need to train a model: download and setup a dataloader, instatiate our model, loss and optizer, and finally run multiple epochs by calling the `train_loop` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "842cdab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(device, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 1000 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def main(device):\n",
    "\n",
    "    # Setup Dataloader\n",
    "    train_dataloader=prepare_data(batch_size=4)\n",
    "    \n",
    "    # Instantiate Model \n",
    "    model = Net().to(device)\n",
    "    \n",
    "    # instantiate loss and optimizer \n",
    "    loss_fn = torch.nn.CrossEntropyLoss() \n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model \n",
    "    epochs = 3\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(device, train_dataloader, model, loss_fn, optimizer)\n",
    "        \n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b92989f",
   "metadata": {},
   "source": [
    "Finally, let's train our model by calling the `main` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d9d67b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "model = main(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2adf5793",
   "metadata": {},
   "source": [
    "### Distributed Code for Multiple GPUs on One Node.\n",
    "\n",
    "Note we will re-use the code from above and modify it to use Pytorch's DDP.  As we mentioned previously there are five main modifications needed to run DDP: \n",
    "\n",
    "1. Create a process group\n",
    "2. Use Pytorch's DistributedSampler to ensure that data passed to each GPU is different\n",
    "3. Wrap Model with Pytorch's DistributedDataParallel\n",
    "4. Modify Training Loop to write model from one GPU\n",
    "5. Close process group\n",
    "\n",
    "The modifications needed for the five changes highlighted above are visually denoted with two lines of `#`.  Note, we reuse the class for `Net` defined in the serial version above.  Note that in the serial code we use the variable `device` to refer to the gpu or cpu we are using to run the code.  In the distributed implementation we will use the variables `local_rank` and `world_size` where:\n",
    "\n",
    "- `local_rank`: the device id of a gpu on one node\n",
    "- `world_size`: the number of gpus on one node.\n",
    "\n",
    "Note, world size will change when we use multiple nodes later on in this course. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6008f81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#################################################\n",
    "# 1. Create a process group (function)\n",
    "def init_distributed(local_rank, world_size):\n",
    "    '''\n",
    "    local_rank: identifier for pariticular GPU on one node\n",
    "    world: total number of process in a the group\n",
    "    '''\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'           # IP address of rank 0 process\n",
    "    os.environ['MASTER_PORT'] = '12355'               # a free port used to communicate amongst processors\n",
    "    torch.cuda.set_device(local_rank)                       #\n",
    "    dist.init_process_group(\"nccl\",                   # backend being used; nccl typically used with distributed GPU training\n",
    "                            rank=local_rank,          # rank of the current process being used\n",
    "                            world_size=world_size)    # total number of processors being used\n",
    "#################################################  \n",
    "    \n",
    "def prepare_data(local_rank, world_size, batch_size=32):\n",
    "\n",
    "    trainset = torchvision.datasets.MNIST(\n",
    "                            root=\"data\",                                        # path to where data is stored\n",
    "                            train=True,                                         # specifies if data is train or test\n",
    "                            download=True,                                      # downloads data if not available at root\n",
    "                            transform=torchvision.transforms.ToTensor()         # trasforms both features and targets accordingly\n",
    "                            )\n",
    "\n",
    "    # pass data to the distributed sampler and dataloader\n",
    "    train_dataloader = DataLoader(trainset,\n",
    "                                  ################################################\n",
    "                                  # 2. Setup Dataloader with Distributed Sampler\n",
    "                                  shuffle=False,\n",
    "                                  sampler=DistributedSampler(trainset, num_replicas=world_size, rank=local_rank),\n",
    "                                  ################################################\n",
    "                                  batch_size=batch_size)\n",
    "\n",
    "    return train_dataloader\n",
    "\n",
    "# training loop for one epoch\n",
    "def train_loop(local_rank, dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # transfer data to GPU if available\n",
    "        X = X.to(local_rank)\n",
    "        y = y.to(local_rank)\n",
    "\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        ################################################\n",
    "        # 4. Only write/print model information on one GPU\n",
    "        if local_rank == 0:\n",
    "            if batch % 100 == 0:\n",
    "                loss, current = loss.item(), batch * len(X)\n",
    "                print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "        ################################################\n",
    "\n",
    "def main(local_rank, world_size):\n",
    "    ################################################\n",
    "    # 1. Set up Process Group\n",
    "    init_distributed(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 2. Setup Dataloader with Distributed Sampler\n",
    "    train_dataloader = prepare_data(local_rank, world_size)\n",
    "    ################################################\n",
    "\n",
    "    ################################################\n",
    "    # 3. Wrap Model with DDP\n",
    "    model = DDP(Net().to(local_rank),\n",
    "        device_ids=[local_rank],                  # list of gpu that model lives on\n",
    "        output_device=local_rank,                 # where to output model\n",
    "    )\n",
    "    ################################################\n",
    "\n",
    "    # instantiate loss and optimizer\n",
    "    loss_fn = torch.nn.CrossEntropyLoss() #torch.nn.MSELoss(reduction='mean')\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    # Train Model\n",
    "    epochs = 10\n",
    "    for t in range(epochs):\n",
    "        print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(local_rank, train_dataloader, model, loss_fn, optimizer)\n",
    "\n",
    "    #################################################\n",
    "    # 5. Close Process Group\n",
    "    dist.destroy_process_group()\n",
    "    #################################################\n",
    "    print(\"Done!\")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d910af48",
   "metadata": {},
   "source": [
    "To run this code in parallel we will utilize the `torch.multiprocessing` package which is a wrapper around Python's native multiprocessing module. In particular, we will use the `spawn` method.  Spawn creates new processes from the parent process but will only inherit the resources necessary to run the `run()` method. Below we highlight the code used to execute training in a python script called `mnist_demo.py` that we will run to execute the parallel implemention of this code.\n",
    "\n",
    "```python\n",
    "if __name__ == \"__main__\":\n",
    "    world_size= torch.cuda.device_count()\n",
    "    print('world_size = {}'.format(world_size))\n",
    "    mp.spawn(main, args=(world_size,) , nprocs=world_size)\n",
    "```\n",
    "\n",
    "Finally we can run train the MNIST classifier using DDP. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd9d7f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python mnist_parallel.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9820b77",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook we covered the basics of how Distributed Data Parallel (DDP) works, highlighted major code modifications needed to convert a serial training script into a distributed training script, and made these modifications for a simple example.  In the next script we will discuss fault tolerance and apply the content covered in this tutorial to the training of the DesignSafe Image Classifier example.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7356ca21",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "There is a script called `simple_linear_regression_serial.py` that implements a simple linear regression model with Pytorch. Modify this script to run on multiple GPUs on one node using Pytorch's DDP. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08beea86",
   "metadata": {},
   "source": [
    "## References \n",
    "\n",
    "1. https://www.nature.com/articles/s41598-021-82543-3\n",
    "2. https://arxiv.org/abs/2006.15704\n",
    "3. https://pytorch.org/tutorials/beginner/ddp_series_theory.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "235c7ae8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
